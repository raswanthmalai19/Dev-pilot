version: '3.8'

services:
  secureai-api:
    build:
      context: ..
      dockerfile: deployment/Dockerfile
      args:
        GPU: "false"  # Set to "true" for GPU support
    container_name: secureai-api
    ports:
      - "8000:8000"
    environment:
      # Server Configuration
      - SECUREAI_HOST=0.0.0.0
      - SECUREAI_PORT=8000
      - SECUREAI_WORKERS=1
      
      # Model Configuration
      - SECUREAI_MODEL_PATH=/models/deepseek-coder-v2-lite-instruct
      - SECUREAI_MODEL_NAME=deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct
      - SECUREAI_MODEL_QUANTIZATION=awq
      - SECUREAI_GPU_MEMORY_UTILIZATION=0.9
      - SECUREAI_TENSOR_PARALLEL_SIZE=1
      
      # Workflow Configuration
      - SECUREAI_MAX_ITERATIONS=3
      - SECUREAI_SYMBOT_TIMEOUT=30
      
      # Logging Configuration
      - SECUREAI_LOG_LEVEL=INFO
      - SECUREAI_LOG_FORMAT=json
      
      # Feature Flags
      - SECUREAI_ENABLE_DOCS=true
      - SECUREAI_ENABLE_GPU=false  # Set to "true" for GPU support
      
      # Rate Limiting
      - SECUREAI_RATE_LIMIT_REQUESTS=10
    volumes:
      # Mount local code directory for development (hot reload)
      - ../api:/app/api:ro
      - ../agent:/app/agent:ro
      - ../__init__.py:/app/__init__.py:ro
      
      # Mount models directory as persistent volume
      - models-data:/models
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    # Uncomment for GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

volumes:
  models-data:
    driver: local
